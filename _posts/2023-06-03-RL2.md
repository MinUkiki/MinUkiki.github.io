---
title : "강화학습 Chapter 2. MarKov Decision Process"
excerpt: "강화학습 Chapter 2. MarKov Decision Process"
categories:
  - AI
tag:
  - AI
toc: true
toc_sticky: true
use_math: true
---

강화학습에서 문제를 잘 정의하려면 주어진 문제를 MDP(MarKov Decision Process)의 형태로 만들어야 한다.

# 1. MarKov Process

![img]({{site.url}}/assets/images/2023-06-03-RL2/image_1.png)

아이가 잠에 들기까지의 과정을 MP로 도식화 한것 ( 수치는 가상으로 설정한 값 )

이 과정을 표현, 정의하는데 필요한 정보는?
1. S, 지금의 상태의 집합 $ S = \{s_0, s_1,s_2,s_3,s_4\} $
2. 화살표에 있는 값, 전의 확률 $ P_{ss'} $

이 정보로 전이 확률 행렬로 만든다면 다음과 같다.

![img]({{site.url}}/assets/images/2023-06-03-RL2/image_2.png)

- $ P_{ss'} $는 상태 $S$에서 다음 상태 $S'$이 될 확률
    - $ P_{ss'} = Pr[S_{t+1}=s' \mid S_t=S] $
    - $ P_{s_1s_0} = 0.1, P_{s_2s_2} = 0 $

## Markov Property

- 미래는 오로지 현재에 의해 결정된다. 즉 과거의 과정은 미래에 영향을 끼치지 않는다를 의미

![img]({{site.url}}/assets/images/2023-06-03-RL2/image_3.png)

- 예를들어 체스 경기가 어느정도 진행되었다고 가정했을때 현재 보드판 모든 상태정보를 관측할 수 있다.
- 다음으로 현재 내가 둬야 하는 상황이라고 가정한다면 미래의 수 즉 현재 내가 둬야 하는 수가 과거에 두었던 수와 연관이 있는가?
- 여기서는 관련이 없다. 이러한 상태를 마르코프 하다라고 정의하겠다. 하지만 다음 예시를 살펴보자

![img]({{site.url}}/assets/images/2023-06-03-RL2/image_4.png)

- 운전중인 사진 한장이 있다. 여기서 어떻게 운전해야 할지 알 수 있을까?
- 물론 사진의 도로위 차는 앞으로 갈것이다. 하지만 여기서 다른 차의 속력을 알수있나? 또한 앞에 차가 전진중인지 후진중인지 확실하게 알수있나?
- 이처럼 현재상태의 정보가 적다면 마르코프 하지않다 라고 한다.
- 여기서 좀 더 마르코프하게 만들기 위해서는 사진 4장 즉 과거의 정보가 주어진다면 현재상태의 정보가 늘어날것이다.

# 2. Markov Reward Process

![img]({{site.url}}/assets/images/2023-06-03-RL2/image_5.png)

- 매 상태에 도달할때 마다 그에 걸맞는 리워드 함수가 추가되었다.

- 이 과정을 정의하는데 필요한 정보는?

1. S, 지금의 상태의 집합 $ S = \{s_0, s_1,s_2,s_3,s_4\} $
2. 화살표에 있는 값, 전의 확률 $ P_{ss'} $

여기에 추가로

3. 보상함수 = $ R = E[r_t\mid S_t = s] $ -> RL에서 최적화 하고자 하는 대상
4. 감쇠인자 $ \gamma $ -> 0에서 1사이의 숫자

=> $ MRP \equiv \{S, P, R, \gamma \} $

## 2-1. 리턴
- 감쇠인자 $ \gamma $를 알기 전에 리턴에 대해 알아보자.
- 리턴은 미래에 받을 리워드의 합
- 어떤 상태의 value를 측정하는데 중요한 의미를 지닌다.

( 이후 더 자세히 배울것 )

>하나의 에피소드를 생각해보자

- $s_T$는 멈춤, r은 리워드
- $ s_0, r_1, s_1, r_2 \ldots , r_T,s_T $

>이때 임의의 t시점에 대해 미래 리워드의 합을 생각해볼 수 있다.

- $ G_t = r_{t+1} +  r_{t+2} + r_{t+3} + r_{t+4} +\ldots $

>이때 미래 시점을 감쇠하는 수식

- $ G_t =  r_{t+1} + \gamma r_{t+2} + \gamma^2r_{t+3} + \gamma^3r_{t+4} + \ldots$

>감쇠인자 $ \gamma $는 왜 필요할까?

1. 수학적 편리성
    - 증명하기 위해서 $ \gamma $가 필요했음
2. 금융에서의 적합성
    - 오늘날의 돈과 10년 후의 만원의 가치를 다르다
3. 사람의 선호 반영
    - 미래에 대한 불확실성

따라서 $ \gamma $ 가 0에 가까울수록 근시안적이고 1에 가까울수록 미래를 중요시한다.

> 참고

- 실제 학습에서는 에피소드 MDP에서만 $ \gamma = 1$ 을 사용한다.
- 에피소드 MDP는 종료상태가 있는 MDP이다. ( 종료가 있기 때문에 수식이 무한대로 발산하지 않음 )

# 3. Markov Decision Process

![img]({{site.url}}/assets/images/2023-06-03-RL2/image_6.png)

- 액션이 추가 됨
- 액션의 집합 $ A = \{a_0, a_1\} $

> 정보


- S : 상태의 집합
    - $ S = \{s_0,s_1,s_2,s_3,s_4\} $

- P : 전이확률
    - $ P^a_{ss'} $
    - s에서 a를 했을때 s'에 도달할 확률
    - $ P^a_{ss'} = P[S_{t+1}=s'\mid S_t=S, a_t = a] $

- R : 보상함수
    - $ R^a_s = E[r_{t+1}\mid S_t = s, a_t = a] $

- $ \gamma $ : 감쇠인자
    - 이전과 동일
    - 0에서 1사이의 값

> 액션이 늘어난다면?


![img]({{site.url}}/assets/images/2023-06-03-RL2/image_7.png)


- 실제 환경은 더욱 복잡하다.

> 마지막으로 MDP를 정리하자면


- MDP는 **상태집합 $ S $**로 이루어지며, **행동집합 $ A $**가 있고, 상태 $ s $에서 행동 $ a $를 통해 상태 $ s' $로 전이할 확률은 **$ P(s' \mid s,a) $** 이며, 그 보상은 **$ R(s,a) $**가 됩니다.

## 3-1. Policy

- Policy fun는 에이전트가 MDP 안에서 어떤 정책을 갖고 움직일지 나타내는 함수
- $ \pi(a\mid s) $ 로 표현
    - 상태s에서 a를 선택할 확률

## 3-2. Value

- Policy는 $ \pi $로 고정되었다 가정
- 에피소드를 샘플링해보면?
    - $ \pi $ 없이 에피소드 샘플링이 가능할까?
- 매번 다른 결과를 얻는다.

- 현재 상태가 얼만큼 좋으냐를 판단하는 것은 미래로 판단할 수 있다.
- 예를들어 시험에 함격했다면 그 시험의 가치는 무엇으로 결정이 되나
- 여기서 말하는 가치는 시험 합격의 가치입니다. 시험에 합격했다면 그로인해 얻는 돈, 명예 등등 따라오기 때문에 시험 합격의 가치가 높은 것 입니다.

- $ \upsilon_\pi(s) =  E_\pi [r_{t+1} + \gamma r_{t+2} + \gamma^2r_{t+3} + \gamma^3r_{t+4} + \mid s_t = s] \\ \qquad  = E_\pi[G_t\mid s_t = s] $ 
- $ \pi $ 를 이용해 상태 s에서 시작해 끝까지 움직일 때 얻게 될 리턴의 기댓값
- 따라서 Value는 $ \pi $가 어떠한 상태에 주어지면 그 상태의 가치를 알려주는 것

## 3-3. State Action Value

- $ q_\pi(s,a) = E_\pi[G_t\mid s,a_t = a] $
- s 에서 a 를 선택하고, 이후에 $ \pi $를 이용해 끝까지 움직일 때 얻게 될 리턴의 값