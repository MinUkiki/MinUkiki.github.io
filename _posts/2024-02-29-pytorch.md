---
title : "Pytorch 개요"
excerpt: "Pytorch 개요"
categories:
  - AI
tag:
  - AI
toc: true
toc_sticky: true
use_math: true
---

# 개요

- Pytorch의 텐서(tensor)는 Numpy 배열과 매우 유사

- Pytorch를 사용하면, GPU 연동을 통해 효율적으로 딥러닝 모델을 학습

간단하게 다음과 같이 Import할 수 있다.

```python
import torch
```

## GPU 사용 여부 체크하기

- 텐서간의 연산을 수행할 때, 기본적으로 두 텐서가 같은 장치에 있어야 함
- 따라서 가능하면, 연상을 수행하는 텐서들을 모두 GPU에 올린 뒤에 연산을 수행

```python
data = [[1,2],[3,4]]

x = torch.tensor(data)
print(x.is_cuda) # false

x = x.cuda() # GPU
print(x.is_cuda) # true

x = x.cpu() # CPU
print(x.is_cuda) # false
```

- 서로 다른 장치에 있는 텐서끼리 연산을 수행하면 오류가 발생

```python
# GPU
a = torch.tensor([[1,1],[2,2]]).cuda()

# CPU
b = torch.tensor([5,6],[7,8])

# print(torch.matmul(a,b)) # 오류발생
print(torch.matmul(a.cpu(),b))
```

# Pytorch의 Tensor

- tensor는 Numpy와 매우 유사함
- 다차원 배열을 처리하기에 적합함
- 자동 미분 기능을 제공

## Tensor의 속성

- shape
- data type
- 저장된 장치

```python
tensor = torch.rand(3,4)

print(tensor)
print(f"Shape : {tensor.shape}")
print(f"Data type : {tensor.dtype}")
print(f"Device : {tensor.device}")

''' output

tensor([[0.4889, 0.0055, 0.6290, 0.0587],
        [0.4900, 0.5542, 0.0827, 0.2913],
        [0.1453, 0.3290, 0.8423, 0.6503]])
Shape : torch.Size([3, 4])
Data type : torch.float32
Device : cpu

'''
```

### Numpy 배열에서 텐서를 초기화 할 수있음

```python
a = torch.tensor([5])
b = torch.tensor([7])

c = (a+b).numpy()
print(c)
print(type(c))

result = c * 10
tensor = torch.from_numpy(result)
print(tensor)
print(type(tensor))

''' output

[12]
<class 'numpy.ndarray'>
tensor([120])
<class 'torch.Tensor'>

'''
```

### 다른 텐서로부터 텐서 초기화하기

```python
x = torch.tensor([[5,7], [1,2]])

# x와 같은 모양과 자료형을 가지지만, 값이 1인 텐서 생성
x_ones = torch.ones_like(x)
print(f"x_ones : {x_ones}")

# x와 같은 모양을 가지되, 자료형을 float으로 덮어쓰고, 값은 랜덤으로 채우기
x_rand = torch.rand_like(x, dtype=torch.float32) # uniform distribution [0,1]
print(f"x_rand : {x_rand}")

''' output

x_ones : tensor([[1, 1],
        [1, 1]])
x_rand : tensor([[0.6189, 0.4604],
        [0.2897, 0.7164]])

'''
```

### 텐서의 형변환 및 차원 조작

- Numpy 처럼 조작 가능

```python
tensor = torch.tensor([[1,2,3,4],[5,6,7,8],[9,10,11,12]])

print(tensor[0]) # first row
print(tensor[:,0]) # first column
print(tensor[...,-1]) # last column

''' output

tensor([1, 2, 3, 4])
tensor([1, 5, 9])
tensor([ 4,  8, 12])
tensor([ 4,  8, 12])

'''
```

- 이어붙이기

```python
# dim : 텐서를 이어 붙이기 위한 축
# 0번 축(행)을 기준으로 이어 붙이기
result = torch.cat([tensor, tensor, tensor], dim=0)
print(f"dim=0 : \n {result}")

# 1번 축(열)을 기준으로 이어 붙이기
result = torch.cat([tensor, tensor, tensor], dim=1)
print(f"dim=1 : \n {result}")

''' output

dim=0 : 
 tensor([[ 1,  2,  3,  4],
        [ 5,  6,  7,  8],
        [ 9, 10, 11, 12],
        [ 1,  2,  3,  4],
        [ 5,  6,  7,  8],
        [ 9, 10, 11, 12],
        [ 1,  2,  3,  4],
        [ 5,  6,  7,  8],
        [ 9, 10, 11, 12]])
dim=1 : 
 tensor([[ 1,  2,  3,  4,  1,  2,  3,  4,  1,  2,  3,  4],
        [ 5,  6,  7,  8,  5,  6,  7,  8,  5,  6,  7,  8],
        [ 9, 10, 11, 12,  9, 10, 11, 12,  9, 10, 11, 12]])

'''
```

- 형변환(tpye casting)

```python

a = torch.tensor([2], dtype=torch.int)
b = torch.tensor([5.0])

print(a.dtype)
print(b.dtype)

# a는 자동으로 형 변환
print(a+b)

# 텐서 b를 변환
print(a+b.type(torch.int32))

''' output

torch.int32
torch.float32
tensor([7.])
tensor([7], dtype=torch.int32)

'''
```

- 텐서의 모양 변경

```python
# view()는 텐서의 모양을 변경할 때 사용
a = torch.tensor([1,2,3,4,5,6,7,8])
b = a.view(4,2)
print(b)
print("-"*20)

# a의 값을 변경하면 b도 변경
a[0] = 15
print(b)
print("-"*20)

# a의 값을 복사한 뒤에 변경
c = a.clone().view(4,2)
a[0]= 9
print(c)

''' output

tensor([[1, 2],
        [3, 4],
        [5, 6],
        [7, 8]])
--------------------
tensor([[15,  2],
        [ 3,  4],
        [ 5,  6],
        [ 7,  8]])
--------------------
tensor([[15,  2],
        [ 3,  4],
        [ 5,  6],
        [ 7,  8]])

'''
```

- 텐서의 차원 교환

```python
a = torch.rand((64,32,3))
print(f"a : {a.shape}")

b = a.permute(2,1,0) # 차원 자체를 교환
print(f"b : {b.shape}")

''' output

a : torch.Size([64, 32, 3])
b : torch.Size([3, 32, 64])

'''
```

### 텐서의 연산과 함수

- 텐서의 연산

```python

tensor = torch.tensor([1,2,3,4,5,6,7,8]).view(4,2)

a = tensor[:2]
b = tensor[2:]
print(f"a : \n{a}")
print(f"b : \n{b}")

print(f"a+b : \n{a+b}")
print(f"a-b : \n{a-b}")
print(f"a*b : \n{a*b}")
print(f"a/b : \n{a/b}")

# print(f"행렬곱 : \n{a.matmul(b)}") # 또는
print(f"행렬곱 : \n{torch.matmul(a,b)}")

''' output

a : 
tensor([[1, 2],
        [3, 4]])
b : 
tensor([[5, 6],
        [7, 8]])
a+b : 
tensor([[ 6,  8],
        [10, 12]])
a-b : 
tensor([[-4, -4],
        [-4, -4]])
a*b : 
tensor([[ 5, 12],
        [21, 32]])
a/b : 
tensor([[0.2000, 0.3333],
        [0.4286, 0.5000]])
행렬곱 : 
tensor([[19, 22],
        [43, 50]])

'''
```

- sum, mean, min, max, 함수

```python

a = torch.Tensor([1,2,3,4,5,6,7,8]).view(2,4)
print(f"a : \n {a}")
print("-"*20)

print(f"a.sum() : \n {a.sum()}") # 전체 원소에 대한 합계
print(f"a.sum(dim=0) : \n {a.sum(dim=0)}") # 각 열에 대한 합계
print(f"a.sum(dim=1) : \n {a.sum(dim=1)}") # 각 행에 대한 합계

print("-"*20)

print(f"a.mean() : \n {a.mean()}") # 전체 원소에 대한 평균
print(f"a.mean(dim=0) : \n {a.mean(dim=0)}") # 각 열에 대한 평균
print(f"a.mean(dim=1) : \n {a.mean(dim=1)}") # 각 행에 대한 평균

print("-"*20)

print(f"a.min() : \n {a.min()}") # 전체 원소에 대한 최솟값
print(f"a.min(dim=0) : \n {a.min(dim=0)}") # 각 열에 대한 최솟값
print(f"a.min(dim=1) : \n {a.min(dim=1)}") # 각 행에 대한 최솟값

print("-"*20)

print(f"a.max() : \n {a.max()}") # 전체 원소에 대한 최댓값
print(f"a.max(dim=0) : \n {a.max(dim=0)}") # 각 열에 대한 최댓값
print(f"a.max(dim=1) : \n {a.max(dim=1)}") # 각 행에 대한 최댓값

# argmax()는 인덱스를 반환

''' output

a : 
 tensor([[1., 2., 3., 4.],
        [5., 6., 7., 8.]])
--------------------
a.sum() : 
 36.0
a.sum(dim=0) : 
 tensor([ 6.,  8., 10., 12.])
a.sum(dim=1) : 
 tensor([10., 26.])
--------------------
a.mean() : 
 4.5
a.mean(dim=0) : 
 tensor([3., 4., 5., 6.])
a.mean(dim=1) : 
 tensor([2.5000, 6.5000])
--------------------
a.min() : 
 1.0
a.min(dim=0) : 
 torch.return_types.min(
values=tensor([1., 2., 3., 4.]),
indices=tensor([0, 0, 0, 0]))
a.min(dim=1) : 
 torch.return_types.min(
values=tensor([1., 5.]),
indices=tensor([0, 0]))
--------------------
a.max() : 
 8.0
a.max(dim=0) : 
 torch.return_types.max(
values=tensor([5., 6., 7., 8.]),
indices=tensor([1, 1, 1, 1]))
a.max(dim=1) : 
 torch.return_types.max(
values=tensor([4., 8.]),
indices=tensor([3, 3]))


'''
```

- unsqueeze, squeeze 함수

```python

a = torch.Tensor([1,2,3,4,5,6,7,8]).view(2,4)
print(f"a.shape : \n {a.shape}")
print("-"*20)

# 첫번째 축에 차원 추가
a = a.unsqueeze(0)
print(f"a : \n {a}")
print(f"a.shape : \n {a.shape}")
print("-"*20)

# 네번째 축에 차원 추가
a = a.unsqueeze(3)
print(f"a : \n {a}")
print(f"a.shape : \n {a.shape}")
print("-"*20)

# 크기가 1인 차원 제거
a = a.squeeze()
print(f"a : \n {a}")
print(f"a.shape : \n {a.shape}")

'''output

a.shape : 
 torch.Size([2, 4])
--------------------
a : 
 tensor([[[1., 2., 3., 4.],
         [5., 6., 7., 8.]]])
a.shape : 
 torch.Size([1, 2, 4])
--------------------
a : 
 tensor([[[[1.],
          [2.],
          [3.],
          [4.]],

         [[5.],
          [6.],
          [7.],
          [8.]]]])
a.shape : 
 torch.Size([1, 2, 4, 1])
--------------------
a : 
 tensor([[1., 2., 3., 4.],
        [5., 6., 7., 8.]])
a.shape : 
 torch.Size([2, 4])

'''
```

### 자동 미분과 기울기

```python
# requires_grad를 설정할 때만 기울기 추적
x = torch.tensor([3.0, 4.0], requires_grad=True)
y = torch.tensor([1.0, 2.0], requires_grad=True)
z = x+y
print(f"z : \n {z}")
print(f"z.grad_fn : \n {z.grad_fn}") # 더하기
print("-"*20)

out = z.mean()
print(f"out : \n {out}")
print(f"out.grad_fn : \n {out.grad_fn}")
print("-"*20)

out.backward() # scalar에 대하여 가능
print(f"x : \n {x.grad}")
print(f"y : \n {y.grad}")
# leaf variable에 대해서만 gradient 추적이 가능
print(f"z : \n {z.grad}") # None

''' output

z : 
 tensor([4., 6.], grad_fn=<AddBackward0>)
z.grad_fn : 
 <AddBackward0 object at 0x0000018CA7A787F0>
--------------------
out : 
 5.0
out.grad_fn : 
 <MeanBackward0 object at 0x0000018CA7A787F0>
--------------------
x : 
 tensor([0.5000, 0.5000])
y : 
 tensor([0.5000, 0.5000])
z : 
 None

'''
```

```python

temp = torch.tensor([3.0, 4.0], requires_grad=True)
print(f"temp.requires_grad : {temp.requires_grad}")
print(f"(temp ** 2).requires_grad : {(temp ** 2).requires_grad}")
print("-"*20)

# 기울기 추적을 하지 않기 때문에 계산 속도가 더 빠름
with torch.no_grad():
    print(f"temp.requires_grad : {temp.requires_grad}")
    print(f"(temp ** 2).requires_grad : {(temp ** 2).requires_grad}")

'''output

temp.requires_grad : True
(temp ** 2).requires_grad : True
--------------------
temp.requires_grad : True
(temp ** 2).requires_grad : False

'''
```